// This is required because the code generated by UniFFI does not satisfy this clippy rule
#![allow(clippy::let_unit_value)]

extern crate core;

pub mod callbacks;
pub mod config;
mod errors;
mod esplora_client;
mod esplora_client_api;
mod event_handler;
mod hex_utils;
mod logger;
mod native_logger;
mod persist;
mod util;

use crate::callbacks::PersistCallback;
use crate::callbacks::RedundantStorageCallback;
use crate::config::LipaLightningConfig;
use crate::errors::LipaLightningError;
use crate::esplora_client::{EsploraClient, EsploraClientSync};
use crate::event_handler::LipaEventHandler;
use crate::hex_utils::to_compressed_pubkey;
use crate::logger::LightningLogger;
use crate::persist::LipaPersister;
use crate::util::{
    connect_peer_if_necessary, do_connect_peer, do_send_payment, get_invoice, keysend, open_channel,
};
use bitcoin::blockdata::constants::genesis_block;
use bitcoin::hashes::hex::ToHex;
use bitcoin::secp256k1::PublicKey;
use bitcoin::{BlockHash, Network, Script, Transaction, Txid};
use lightning::chain;
use lightning::chain::keysinterface::{InMemorySigner, KeysInterface, KeysManager, Recipient};
use lightning::chain::{chainmonitor, BestBlock, Filter, WatchedOutput};
use lightning::ln::channelmanager::{
    ChainParameters, ChannelManagerReadArgs, SimpleArcChannelManager,
};
use lightning::ln::peer_handler::{IgnoringMessageHandler, MessageHandler, SimpleArcPeerManager};
use lightning::ln::{PaymentHash, PaymentPreimage, PaymentSecret};
use lightning::routing::gossip;
use lightning::routing::gossip::P2PGossipSync;
use lightning::routing::scoring::{ProbabilisticScorer, ProbabilisticScoringParameters};
use lightning::util::config::UserConfig;
use lightning::util::ser::ReadableArgs;
use lightning_background_processor::{BackgroundProcessor, GossipSync};
use lightning_invoice::utils::DefaultRouter;
use lightning_invoice::{payment, Invoice};
use lightning_net_tokio::SocketDescriptor;
use log::{error, info, Level as LogLevel};
use rand::Rng;
use std::collections::HashMap;
use std::net::SocketAddr;
use std::str::FromStr;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Arc, Mutex};
use std::thread::sleep;
use std::time::{Duration, SystemTime};
use std::{fmt, thread};
use tokio::runtime::{Handle, Runtime};

pub type TransactionWithPosition = (usize, Transaction);

pub struct LipaLightning {
    tokio_runtime: Runtime,
    ldk: LipaLdk,
    persist_callback: Arc<Box<dyn PersistCallback>>,
}
/// The main 3L "class"
///
/// Creating an instance initializes all Lightning related background tasks.
///
/// For a safe shutdown of the Lightning node, `stop()` must be called before terminating the app.
impl LipaLightning {
    /// Returns a LipaLightning instance
    ///
    /// Creates a tokio async runtime and spawns all background tasks necessary for a functioning
    /// Lightning node
    ///
    /// # Arguments
    ///
    /// * `config` - A LipaLightningConfig instance that provides several configuration parameters
    /// * `persist_callback` - A PersistCallback implementation
    pub fn new(config: LipaLightningConfig, persist_callback: Box<dyn PersistCallback>) -> Self {
        let tokio_runtime = tokio::runtime::Builder::new_multi_thread()
            .enable_all()
            .build()
            .unwrap();

        let arc_persist_callback = Arc::new(persist_callback);

        let ldk =
            LipaLdk::new(config, tokio_runtime.handle(), arc_persist_callback.clone()).unwrap();

        LipaLightning {
            tokio_runtime,
            ldk,
            persist_callback: arc_persist_callback,
        }
    }

    /// Graceful shutdown procedure for 3L
    ///
    /// Disconnect our peers and stop accepting new connections. Ensures we don't continue updating
    /// our channel data after stopping the background processor
    pub fn stop(&self) {
        self.ldk.stop_listen_connect.store(true, Ordering::Release);
        self.ldk.peer_manager.disconnect_all_peers();
    }

    /// Get our own node id
    pub fn get_my_node_id(&self) -> Vec<u8> {
        self.ldk
            .channel_manager
            .get_our_node_id()
            .serialize()
            .to_vec()
    }

    pub fn get_node_info(&self) -> LipaNodeInfo {
        let chans = self.ldk.channel_manager.list_channels();
        let local_balance_msat = chans.iter().map(|c| c.balance_msat).sum::<u64>();
        LipaNodeInfo {
            node_pubkey: self
                .ldk
                .channel_manager
                .get_our_node_id()
                .serialize()
                .to_vec(),
            num_channels: chans.len() as u64,
            num_usable_channels: chans.iter().filter(|c| c.is_usable).count() as u64,
            local_balance_msat,
            num_peers: self.ldk.peer_manager.get_peer_node_ids().len() as u64,
        }
    }

    /// Connect to a node and open a new channel. Disconnects and re-connects are handled
    /// automatically
    pub fn connect_open_channel(
        &self,
        node_id: Vec<u8>,
        node_address: String,
        channel_value_sat: u64,
    ) -> Result<(), LipaLightningError> {
        let pubkey = PublicKey::from_slice(&*node_id).unwrap();
        let peer_addr = SocketAddr::from_str(&*node_address).unwrap();

        self.tokio_runtime.block_on(connect_peer_if_necessary(
            pubkey,
            peer_addr,
            self.ldk.peer_manager.clone(),
        ))?;

        open_channel(
            pubkey,
            channel_value_sat,
            false,
            self.ldk.channel_manager.clone(),
        )?;

        // Persist peer address for reconnecting in the future
        let peer_data_path = format!("channel_peer_data/{}", pubkey);
        self.persist_callback
            .write_to_file(peer_data_path, peer_addr.to_string().into_bytes());

        Ok(())
    }

    /// Close a previously opened channel
    pub fn close_channel(&self, _channel_id: u64) -> Result<(), LipaLightningError> {
        todo!()
    }

    /*/// Start the pathfinding process for a potential future payment.
    ///
    /// This method should be called as soon as an invoice is available, even if later the user ends
    /// up not wanting to make the payment. This way, the perceived payment time will be much lower.
    pub fn start_pathfinding(&self, _invoice: String) -> Result<(), ()> { }*/

    /// Pay a BOLT11 invoice
    ///
    /// # Arguments
    /// * `invoice_str` - Invoice string
    pub fn send_payment(&self, invoice_str: String) -> Result<(), LipaLightningError> {
        let invoice = match Invoice::from_str(invoice_str.as_str()) {
            Ok(inv) => inv,
            Err(_) => {
                error!("Failed to parse BOLT11 invoice");
                return Err(LipaLightningError::InvoiceParsing);
            }
        };

        do_send_payment(
            &*self.ldk.invoice_payer,
            &invoice,
            self.ldk.outbound_payments.clone(),
        )?;

        Ok(())
    }

    /// Send a spontaneous payment (aka "Keysend")
    ///
    /// # Arguments
    /// * `amount_msat` - Invoice amount in millisatoshis
    /// * `node_id` - Payee node id
    pub fn send_spontaneous_payment(
        &self,
        amount_msat: u64,
        node_id: Vec<u8>,
    ) -> Result<(), LipaLightningError> {
        let pubkey = match PublicKey::from_slice(&*node_id) {
            Ok(pk) => pk,
            Err(_) => {
                return Err(LipaLightningError::PubkeyParsing {
                    pubkey: node_id.to_hex(),
                });
            }
        };

        keysend(
            self.ldk.invoice_payer.clone(),
            pubkey,
            amount_msat,
            self.ldk.keys_manager.clone(),
            self.ldk.outbound_payments.clone(),
        )?;

        Ok(())
    }

    /// Create an invoice to receive a payment
    ///
    /// # Arguments
    /// * `amount_msat` - Invoice amount in millisatoshis
    /// * `expiry_secs` - Invoice duration in seconds
    pub fn create_invoice(
        &self,
        amount_msat: u64,
        expiry_secs: u32,
    ) -> Result<String, LipaLightningError> {
        get_invoice(
            amount_msat,
            self.ldk.inbound_payments.clone(),
            self.ldk.channel_manager.clone(),
            self.ldk.keys_manager.clone(),
            self.ldk.network,
            expiry_secs,
        )
    }

    // TODO: Some other methods are missing here
    // (e.g. get onchain info, channel info, payment info...)
}

pub struct LipaNodeInfo {
    pub node_pubkey: Vec<u8>,
    pub num_channels: u64,
    pub num_usable_channels: u64,
    pub local_balance_msat: u64,
    pub num_peers: u64,
}

type ChainMonitor = chainmonitor::ChainMonitor<
    InMemorySigner,
    Arc<dyn Filter + Send + Sync>,
    Arc<EsploraClient>,
    Arc<EsploraClient>,
    Arc<LightningLogger>,
    Arc<LipaPersister>,
>;

pub(crate) type ChannelManager =
    SimpleArcChannelManager<ChainMonitor, EsploraClient, EsploraClient, LightningLogger>;

pub(crate) type PeerManager = SimpleArcPeerManager<
    SocketDescriptor,
    ChainMonitor,
    EsploraClient,
    EsploraClient,
    dyn chain::Access + Send + Sync,
    LightningLogger,
>;

pub(crate) enum HTLCStatus {
    Pending,
    Succeeded,
    Failed,
}

pub(crate) struct MillisatAmount(Option<u64>);

impl fmt::Display for MillisatAmount {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self.0 {
            Some(amt) => write!(f, "{}", amt),
            None => write!(f, "unknown"),
        }
    }
}

pub(crate) struct PaymentInfo {
    preimage: Option<PaymentPreimage>,
    secret: Option<PaymentSecret>,
    status: HTLCStatus,
    amt_msat: MillisatAmount,
}

pub(crate) type PaymentInfoStorage = Arc<Mutex<HashMap<PaymentHash, PaymentInfo>>>;

type Router = DefaultRouter<Arc<NetworkGraph>, Arc<LightningLogger>>;

pub(crate) type NetworkGraph = gossip::NetworkGraph<Arc<LightningLogger>>;

pub(crate) type InvoicePayer = payment::InvoicePayer<
    Arc<ChannelManager>,
    Router,
    Arc<Mutex<ProbabilisticScorer<Arc<NetworkGraph>, Arc<LightningLogger>>>>,
    Arc<LightningLogger>,
    Arc<LipaEventHandler>,
>;

#[allow(dead_code)]
struct LipaLdk {
    stop_listen_connect: Arc<AtomicBool>,
    peer_manager: Arc<PeerManager>,
    channel_manager: Arc<ChannelManager>,
    chain_monitor: Arc<ChainMonitor>,
    background_processor: BackgroundProcessor,
    invoice_payer: Arc<InvoicePayer>,
    inbound_payments: PaymentInfoStorage,
    outbound_payments: PaymentInfoStorage,
    network_graph: Arc<NetworkGraph>,
    keys_manager: Arc<KeysManager>,
    logger: Arc<LightningLogger>,
    scorer: Arc<Mutex<ProbabilisticScorer<Arc<NetworkGraph>, Arc<LightningLogger>>>>,
    network: Network,
}

// stuff we are interested in to watch on the chain (since we don't watch everything)
struct TxFilter {
    watched_transactions: Vec<(Txid, Script)>,
    watched_outputs: Vec<WatchedOutput>,
}

impl TxFilter {
    fn new() -> Self {
        Self {
            watched_transactions: vec![],
            watched_outputs: vec![],
        }
    }

    fn register_tx(&mut self, txid: Txid, script: Script) {
        println!("Registering tx: {}", txid);
        self.watched_transactions.push((txid, script));
    }

    fn register_output(&mut self, output: WatchedOutput) {
        println!("watching output: {}", output.outpoint.txid);
        self.watched_outputs.push(output);
    }
}

impl Default for TxFilter {
    fn default() -> Self {
        Self::new()
    }
}

impl LipaLdk {
    /// Creates a new LDK instance
    ///
    /// Several tasks are launched into the tokio runtime handle that is provided.
    fn new(
        config: LipaLightningConfig,
        tokio_handle: &Handle,
        persist_callback: Arc<Box<dyn PersistCallback>>,
    ) -> Result<Self, ()> {
        let client = Arc::new(EsploraClient::new(&config.get_esplora_url()));

        // ## Setup

        info!("Setting up the node");

        // Step 1: Initialize the FeeEstimator
        let fee_estimator = Arc::new(EsploraClient::new(&config.get_esplora_url()));

        // Step 2: Initialize the Logger
        let logger = Arc::new(LightningLogger {});

        // Step 3: Initialize the BroadcasterInterface
        let broadcaster = client.clone();

        // Step 4: Initialize Persist
        let persister = Arc::new(LipaPersister::new(persist_callback.clone()));

        // Step 5: (Optional) Initialize the Transaction Filter
        let filter = Arc::new(ChainFilter::new());

        // Step 6: Initialize the ChainMonitor
        let chain_monitor: Arc<ChainMonitor> = Arc::new(ChainMonitor::new(
            Some(filter.clone()),
            broadcaster.clone(),
            logger.clone(),
            fee_estimator.clone(),
            persister.clone(),
        ));

        // Step 7: Initialize the KeysManager using our seed
        if config.seed.len() != 32 {
            error!("Seed should have 32 bytes but has {}", config.seed.len());
        }
        let seed = <&[u8; 32]>::try_from(&*config.seed).unwrap();
        let cur = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap();
        let keys_manager = Arc::new(KeysManager::new(seed, cur.as_secs(), cur.subsec_nanos()));

        // Step 8: Read ChannelMonitor state from disk
        let mut channelmonitors = persister
            .read_channelmonitors(keys_manager.clone())
            .unwrap();

        // Step 9: Initialize the ChannelManager
        let mut user_config = UserConfig::default();

        // Do not announce outbound channels
        user_config.channel_handshake_config.announced_channel = false;
        // Enforce inbound channels to be unannounced channels
        user_config
            .channel_handshake_limits
            .force_announced_channel_preference = true;

        // todo implement accepting 0-conf inbound channels
        // required for 0-conf channels
        // user_config.manually_accept_inbound_channels = true;

        let channel_manager = {
            if persist_callback.exists("manager".to_string()) {
                let mut channel_monitor_mut_references = Vec::new();
                for (_, channel_monitor) in channelmonitors.iter_mut() {
                    channel_monitor_mut_references.push(channel_monitor);
                }
                let read_args = ChannelManagerReadArgs::new(
                    keys_manager.clone(),
                    fee_estimator.clone(),
                    chain_monitor.clone(),
                    broadcaster.clone(),
                    logger.clone(),
                    user_config,
                    channel_monitor_mut_references,
                );
                let mut ser_channelmanager = &*persist_callback.read("manager".to_string());
                let cm_persisted =
                    <(BlockHash, ChannelManager)>::read(&mut ser_channelmanager, read_args)
                        .unwrap();
                cm_persisted.1
            } else {
                // We're starting a fresh node.

                let tip_block_height = tokio_handle.block_on(client.get_height()).unwrap();
                let tip_block_hash = tokio_handle
                    .block_on(client.get_block_hash(tip_block_height))
                    .unwrap();

                let chain_params = ChainParameters {
                    network: config.network,
                    best_block: BestBlock::new(tip_block_hash, tip_block_height),
                };

                ChannelManager::new(
                    fee_estimator.clone(),
                    chain_monitor.clone(),
                    broadcaster.clone(),
                    logger.clone(),
                    keys_manager.clone(),
                    user_config,
                    chain_params,
                )
            }
        };

        // Step 12: (Optional) Initialize the NetGraphMsgHandler
        let genesis = genesis_block(config.network).header.block_hash();
        let network_graph_path = "network_graph".to_string();
        let network_graph = if !persist_callback.exists(network_graph_path.clone()) {
            Arc::new(NetworkGraph::new(genesis, logger.clone()))
        } else {
            let mut ser_network_graph = &*persist_callback.read(network_graph_path);
            Arc::new(NetworkGraph::read(&mut ser_network_graph, logger.clone()).unwrap())
        };
        let gossip_sync = Arc::new(P2PGossipSync::new(
            Arc::clone(&network_graph),
            None::<Arc<dyn chain::Access + Send + Sync>>,
            logger.clone(),
        ));

        // Step 13: Initialize the PeerManager
        let channel_manager: Arc<ChannelManager> = Arc::new(channel_manager);
        let mut ephemeral_bytes = [0; 32];
        rand::thread_rng().fill_bytes(&mut ephemeral_bytes);
        let lightning_msg_handler = MessageHandler {
            chan_handler: channel_manager.clone(),
            route_handler: gossip_sync.clone(),
        };
        let peer_manager: Arc<PeerManager> = Arc::new(PeerManager::new(
            lightning_msg_handler,
            keys_manager.get_node_secret(Recipient::Node).unwrap(),
            &ephemeral_bytes,
            logger.clone(),
            Arc::new(IgnoringMessageHandler {}),
        ));

        // ## Running LDK

        // Step 14: Initialize Networking
        let peer_manager_connection_handler = peer_manager.clone();
        let listening_port = config.ldk_peer_listening_port;
        let stop_listen_connect = Arc::new(AtomicBool::new(false));
        let stop_listen = Arc::clone(&stop_listen_connect);
        tokio_handle.spawn(async move {
            let listener = tokio::net::TcpListener::bind(format!("0.0.0.0:{}", listening_port))
                .await
                .expect(
                    "Failed to bind to listen port - is something else already listening on it?",
                );
            loop {
                let peer_mgr = peer_manager_connection_handler.clone();
                let tcp_stream = listener.accept().await.unwrap().0;
                if stop_listen.load(Ordering::Acquire) {
                    return;
                }
                tokio::spawn(async move {
                    lightning_net_tokio::setup_inbound(
                        peer_mgr.clone(),
                        tcp_stream.into_std().unwrap(),
                    )
                    .await;
                });
            }
        });

        // Step 15: Keep LDK Up-to-date with Chain Info
        let channel_manager_sync = channel_manager.clone();
        let chain_monitor_sync = chain_monitor.clone();
        let filter_sync = filter.clone();
        let esplora_url_sync = config.get_esplora_url().clone();
        /*tokio_handle.spawn(async move {
            let esplora_client = EsploraClientSync::new(
                esplora_url_sync,
                filter_sync,
                channel_manager_sync,
                chain_monitor_sync,
            );
            loop {
                esplora_client.sync().await;
                tokio::time::sleep(Duration::from_secs(1)).await;
            }
        });*/
        let _sync_handle = thread::spawn(move || {
            let runtime = tokio::runtime::Runtime::new().unwrap();
            let esplora_client = EsploraClientSync::new(
                esplora_url_sync,
                filter_sync,
                channel_manager_sync,
                chain_monitor_sync,
            );
            loop {
                //println!("Calling sync");
                match runtime.block_on(esplora_client.sync()) {
                    Ok(_) => {}
                    Err(_) => {
                        error!("esplora_client.sync() failed")
                    }
                };
                sleep(Duration::from_secs(1));
            }
        });

        // Step 16: Initialize an EventHandler
        // TODO: persist payment info to disk
        let inbound_payments: PaymentInfoStorage = Arc::new(Mutex::new(HashMap::new()));
        let outbound_payments: PaymentInfoStorage = Arc::new(Mutex::new(HashMap::new()));
        let network = config.network;
        let event_handler = Arc::new(LipaEventHandler {
            channel_manager: channel_manager.clone(),
            network_graph: network_graph.clone(),
            inbound_payments: inbound_payments.clone(),
            outbound_payments: outbound_payments.clone(),
            network,
            tokio_handle: tokio_handle.clone(),
        });

        // Step 17: Initialize the ProbabilisticScorer
        let scorer_path = "scorer".to_string();
        let scorer = if !persist_callback.exists(scorer_path.clone()) {
            Arc::new(Mutex::new(ProbabilisticScorer::new(
                ProbabilisticScoringParameters::default(),
                network_graph.clone(),
                logger.clone(),
            )))
        } else {
            let mut ser_scorer = &*persist_callback.read(scorer_path);
            let args = (
                ProbabilisticScoringParameters::default(),
                network_graph.clone(),
                logger.clone(),
            );
            Arc::new(Mutex::new(
                ProbabilisticScorer::read(&mut ser_scorer, args).unwrap(),
            ))
        };

        // Step 18: Initialize the InvoicePayer
        let router = DefaultRouter::new(
            network_graph.clone(),
            logger.clone(),
            keys_manager.get_secure_random_bytes(),
        );
        let invoice_payer = Arc::new(InvoicePayer::new(
            channel_manager.clone(),
            router,
            scorer.clone(),
            logger.clone(),
            event_handler,
            payment::Retry::Timeout(Duration::from_secs(10)),
        ));

        // Step 19: Initialize the Persister
        let persister = Arc::new(LipaPersister::new(persist_callback.clone()));

        // Step 20: Start background processor
        let background_processor = BackgroundProcessor::start(
            persister,
            invoice_payer.clone(),
            chain_monitor.clone(),
            channel_manager.clone(),
            GossipSync::p2p(gossip_sync.clone()),
            peer_manager.clone(),
            logger.clone(),
            Some(scorer.clone()),
        );

        // Regularly reconnect to channel peers.
        let connect_cm = Arc::clone(&channel_manager);
        let connect_pm = Arc::clone(&peer_manager);
        let peer_data_path = "channel_peer_data".to_string();
        let stop_connect = Arc::clone(&stop_listen_connect);
        let connect_pc = Arc::clone(&persist_callback);
        tokio_handle.spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(1));
            loop {
                interval.tick().await;
                // read channel peer data from disk
                if !connect_pc.exists(peer_data_path.clone()) {
                    continue;
                }
                let peer_pubkeys_str = connect_pc.read_dir(peer_data_path.clone());
                let peer_pubkeys: Vec<(PublicKey, String)> = peer_pubkeys_str
                    .iter()
                    .map(|pubkey_str| {
                        (
                            to_compressed_pubkey(pubkey_str).unwrap(),
                            pubkey_str.clone(),
                        )
                    })
                    .collect();

                let peers = connect_pm.get_peer_node_ids();
                for node_id in connect_cm
                    .list_channels()
                    .iter()
                    .map(|chan| chan.counterparty.node_id)
                    .filter(|id| !peers.contains(id))
                {
                    if stop_connect.load(Ordering::Acquire) {
                        return;
                    }
                    for (pubkey, pubkey_str) in peer_pubkeys.iter() {
                        if *pubkey == node_id {
                            let peer_addr_vec =
                                connect_pc.read(format!("{}/{}", peer_data_path, pubkey_str));
                            let peer_addr_str = &*String::from_utf8(peer_addr_vec)
                                .expect("ERROR: Failed to parse peer_addr into String");
                            let peer_addr = SocketAddr::from_str(peer_addr_str).expect(
                                "ERROR: Failed to parse peer_addr from String to SocketAddr",
                            );
                            let _ = do_connect_peer(*pubkey, peer_addr, connect_pm.clone()).await;
                        }
                    }
                }
            }
        });

        // We won't have public channels so we don't need to broadcast our node_announcement

        Ok(LipaLdk {
            stop_listen_connect,
            peer_manager,
            channel_manager,
            chain_monitor,
            background_processor,
            invoice_payer,
            inbound_payments,
            outbound_payments,
            network_graph,
            keys_manager,
            logger,
            scorer,
            network,
        })
    }
}

struct ChainFilter {
    filter: Mutex<TxFilter>,
}

impl ChainFilter {
    fn new() -> Self {
        Self {
            filter: Mutex::new(TxFilter::new()),
        }
    }
}

impl Filter for ChainFilter {
    fn register_tx(&self, txid: &Txid, script_pubkey: &Script) {
        let mut filter = self.filter.lock().unwrap();
        filter.register_tx(*txid, script_pubkey.clone());
    }

    fn register_output(&self, output: WatchedOutput) -> Option<TransactionWithPosition> {
        let mut filter = self.filter.lock().unwrap();
        filter.register_output(output);
        // TODO: do we need to check for tx here or wait for next sync?
        None
    }
}

pub fn init_native_logger_once(min_level: LogLevel) {
    native_logger::init_native_logger_once(min_level);
}

include!(concat!(env!("OUT_DIR"), "/lipalightninglib.uniffi.rs"));
