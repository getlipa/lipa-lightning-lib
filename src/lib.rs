// This is required because the code generated by UniFFI does not satisfy this clippy rule
#![allow(clippy::let_unit_value)]

extern crate core;

pub mod callbacks;
pub mod config;
mod errors;
mod esplora_client;
mod esplora_client_api;
mod event_handler;
mod hex_utils;
mod logger;
mod native_logger;
mod persist;
mod util;

use crate::callbacks::PersistCallback;
use crate::callbacks::RedundantStorageCallback;
use crate::config::LipaLightningConfig;
use crate::errors::LipaLightningError;
use crate::esplora_client::EsploraClient;
use crate::esplora_client_api::Error;
use crate::event_handler::LipaEventHandler;
use crate::hex_utils::to_compressed_pubkey;
use crate::logger::LightningLogger;
use crate::persist::LipaPersister;
use crate::util::{
    connect_peer_if_necessary, do_connect_peer, do_send_payment, get_invoice, keysend, open_channel,
};
use bitcoin::blockdata::constants::genesis_block;
use bitcoin::hashes::hex::ToHex;
use bitcoin::secp256k1::PublicKey;
use bitcoin::{BlockHash, Network, Script, Transaction, Txid};
use lightning::chain;
use lightning::chain::keysinterface::{InMemorySigner, KeysInterface, KeysManager, Recipient};
use lightning::chain::{chainmonitor, BestBlock, Confirm, Filter, WatchedOutput};
use lightning::ln::channelmanager::{
    ChainParameters, ChannelManagerReadArgs, SimpleArcChannelManager,
};
use lightning::ln::peer_handler::{IgnoringMessageHandler, MessageHandler, SimpleArcPeerManager};
use lightning::ln::{PaymentHash, PaymentPreimage, PaymentSecret};
use lightning::routing::gossip;
use lightning::routing::gossip::P2PGossipSync;
use lightning::routing::scoring::{ProbabilisticScorer, ProbabilisticScoringParameters};
use lightning::util::config::UserConfig;
use lightning::util::ser::ReadableArgs;
use lightning_background_processor::{BackgroundProcessor, GossipSync};
use lightning_invoice::utils::DefaultRouter;
use lightning_invoice::{payment, Invoice};
use lightning_net_tokio::SocketDescriptor;
use log::{error, info, Level as LogLevel};
use rand::Rng;
use std::collections::HashMap;
use std::fmt;
use std::net::SocketAddr;
use std::str::FromStr;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Arc, Mutex};
use std::time::{Duration, SystemTime};
use tokio::runtime::{Handle, Runtime};

pub type TransactionWithPosition = (usize, Transaction);

pub struct LipaLightning {
    tokio_runtime: Runtime,
    ldk: LipaLdk,
    persist_callback: Arc<Box<dyn PersistCallback>>,
}
/// The main 3L "class"
///
/// Creating an instance initializes all Lightning related background tasks.
///
/// For a safe shutdown of the Lightning node, `stop()` must be called before terminating the app.
impl LipaLightning {
    /// Returns a LipaLightning instance
    ///
    /// Creates a tokio async runtime and spawns all background tasks necessary for a functioning
    /// Lightning node
    ///
    /// # Arguments
    ///
    /// * `config` - A LipaLightningConfig instance that provides several configuration parameters
    /// * `persist_callback` - A PersistCallback implementation
    pub fn new(config: LipaLightningConfig, persist_callback: Box<dyn PersistCallback>) -> Self {
        let tokio_runtime = tokio::runtime::Builder::new_multi_thread()
            .enable_all()
            .build()
            .unwrap();

        let arc_persist_callback = Arc::new(persist_callback);

        let ldk =
            LipaLdk::new(config, tokio_runtime.handle(), arc_persist_callback.clone()).unwrap();

        LipaLightning {
            tokio_runtime,
            ldk,
            persist_callback: arc_persist_callback,
        }
    }

    /// Graceful shutdown procedure for 3L
    ///
    /// Disconnect our peers and stop accepting new connections. Ensures we don't continue updating
    /// our channel data after stopping the background processor
    pub fn stop(&self) {
        self.ldk.stop_listen_connect.store(true, Ordering::Release);
        self.ldk.peer_manager.disconnect_all_peers();
    }

    /// Get our own node id
    pub fn get_my_node_id(&self) -> Vec<u8> {
        self.ldk
            .channel_manager
            .get_our_node_id()
            .serialize()
            .to_vec()
    }

    pub fn get_node_info(&self) -> LipaNodeInfo {
        let chans = self.ldk.channel_manager.list_channels();
        let local_balance_msat = chans.iter().map(|c| c.balance_msat).sum::<u64>();
        LipaNodeInfo {
            node_pubkey: self
                .ldk
                .channel_manager
                .get_our_node_id()
                .serialize()
                .to_vec(),
            num_channels: chans.len() as u64,
            num_usable_channels: chans.iter().filter(|c| c.is_usable).count() as u64,
            local_balance_msat,
            num_peers: self.ldk.peer_manager.get_peer_node_ids().len() as u64,
        }
    }

    /// Connect to a node and open a new channel. Disconnects and re-connects are handled
    /// automatically
    pub fn connect_open_channel(
        &self,
        node_id: Vec<u8>,
        node_address: String,
        channel_value_sat: u64,
    ) -> Result<(), LipaLightningError> {
        let pubkey = PublicKey::from_slice(&*node_id).unwrap();
        let peer_addr = SocketAddr::from_str(&*node_address).unwrap();

        self.tokio_runtime.block_on(connect_peer_if_necessary(
            pubkey,
            peer_addr,
            self.ldk.peer_manager.clone(),
        ))?;

        open_channel(
            pubkey,
            channel_value_sat,
            false,
            self.ldk.channel_manager.clone(),
        )?;

        // Persist peer address for reconnecting in the future
        let peer_data_path = format!("channel_peer_data/{}", pubkey);
        self.persist_callback
            .write_to_file(peer_data_path, peer_addr.to_string().into_bytes());

        Ok(())
    }

    /// Close a previously opened channel
    pub fn close_channel(&self, _channel_id: u64) -> Result<(), LipaLightningError> {
        todo!()
    }

    /*/// Start the pathfinding process for a potential future payment.
    ///
    /// This method should be called as soon as an invoice is available, even if later the user ends
    /// up not wanting to make the payment. This way, the perceived payment time will be much lower.
    pub fn start_pathfinding(&self, _invoice: String) -> Result<(), ()> { }*/

    /// Pay a BOLT11 invoice
    ///
    /// # Arguments
    /// * `invoice_str` - Invoice string
    pub fn send_payment(&self, invoice_str: String) -> Result<(), LipaLightningError> {
        let invoice = match Invoice::from_str(invoice_str.as_str()) {
            Ok(inv) => inv,
            Err(_) => {
                error!("Failed to parse BOLT11 invoice");
                return Err(LipaLightningError::InvoiceParsing);
            }
        };

        do_send_payment(
            &*self.ldk.invoice_payer,
            &invoice,
            self.ldk.outbound_payments.clone(),
        )?;

        Ok(())
    }

    /// Send a spontaneous payment (aka "Keysend")
    ///
    /// # Arguments
    /// * `amount_msat` - Invoice amount in millisatoshis
    /// * `node_id` - Payee node id
    pub fn send_spontaneous_payment(
        &self,
        amount_msat: u64,
        node_id: Vec<u8>,
    ) -> Result<(), LipaLightningError> {
        let pubkey = match PublicKey::from_slice(&*node_id) {
            Ok(pk) => pk,
            Err(_) => {
                return Err(LipaLightningError::PubkeyParsing {
                    pubkey: node_id.to_hex(),
                });
            }
        };

        keysend(
            self.ldk.invoice_payer.clone(),
            pubkey,
            amount_msat,
            self.ldk.keys_manager.clone(),
            self.ldk.outbound_payments.clone(),
        )?;

        Ok(())
    }

    /// Create an invoice to receive a payment
    ///
    /// # Arguments
    /// * `amount_msat` - Invoice amount in millisatoshis
    /// * `expiry_secs` - Invoice duration in seconds
    pub fn create_invoice(
        &self,
        amount_msat: u64,
        expiry_secs: u32,
    ) -> Result<String, LipaLightningError> {
        get_invoice(
            amount_msat,
            self.ldk.inbound_payments.clone(),
            self.ldk.channel_manager.clone(),
            self.ldk.keys_manager.clone(),
            self.ldk.network,
            expiry_secs,
        )
    }

    pub fn sync(&self) {
        if let Err(e) = self.tokio_runtime.block_on(self.ldk.sync()) {
            error!("Syncing Error: {}", e);
        }
    }

    // TODO: Some other methods are missing here
    // (e.g. get onchain info, channel info, payment info...)
}

pub struct LipaNodeInfo {
    pub node_pubkey: Vec<u8>,
    pub num_channels: u64,
    pub num_usable_channels: u64,
    pub local_balance_msat: u64,
    pub num_peers: u64,
}

type ChainMonitor = chainmonitor::ChainMonitor<
    InMemorySigner,
    Arc<dyn Filter + Send + Sync>,
    Arc<EsploraClient>,
    Arc<EsploraClient>,
    Arc<LightningLogger>,
    Arc<LipaPersister>,
>;

pub(crate) type ChannelManager =
    SimpleArcChannelManager<ChainMonitor, EsploraClient, EsploraClient, LightningLogger>;

pub(crate) type PeerManager = SimpleArcPeerManager<
    SocketDescriptor,
    ChainMonitor,
    EsploraClient,
    EsploraClient,
    dyn chain::Access + Send + Sync,
    LightningLogger,
>;

pub(crate) enum HTLCStatus {
    Pending,
    Succeeded,
    Failed,
}

pub(crate) struct MillisatAmount(Option<u64>);

impl fmt::Display for MillisatAmount {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self.0 {
            Some(amt) => write!(f, "{}", amt),
            None => write!(f, "unknown"),
        }
    }
}

pub(crate) struct PaymentInfo {
    preimage: Option<PaymentPreimage>,
    secret: Option<PaymentSecret>,
    status: HTLCStatus,
    amt_msat: MillisatAmount,
}

pub(crate) type PaymentInfoStorage = Arc<Mutex<HashMap<PaymentHash, PaymentInfo>>>;

type Router = DefaultRouter<Arc<NetworkGraph>, Arc<LightningLogger>>;

pub(crate) type NetworkGraph = gossip::NetworkGraph<Arc<LightningLogger>>;

pub(crate) type InvoicePayer = payment::InvoicePayer<
    Arc<ChannelManager>,
    Router,
    Arc<Mutex<ProbabilisticScorer<Arc<NetworkGraph>, Arc<LightningLogger>>>>,
    Arc<LightningLogger>,
    Arc<LipaEventHandler>,
>;

#[allow(dead_code)]
struct LipaLdk {
    client: Arc<EsploraClient>,
    stop_listen_connect: Arc<AtomicBool>,
    peer_manager: Arc<PeerManager>,
    channel_manager: Arc<ChannelManager>,
    chain_monitor: Arc<ChainMonitor>,
    background_processor: BackgroundProcessor,
    invoice_payer: Arc<InvoicePayer>,
    inbound_payments: PaymentInfoStorage,
    outbound_payments: PaymentInfoStorage,
    network_graph: Arc<NetworkGraph>,
    keys_manager: Arc<KeysManager>,
    logger: Arc<LightningLogger>,
    scorer: Arc<Mutex<ProbabilisticScorer<Arc<NetworkGraph>, Arc<LightningLogger>>>>,
    network: Network,
    filter: Arc<ChainFilter>,
}

// stuff we are interested in to watch on the chain (since we don't watch everything)
struct TxFilter {
    watched_transactions: Vec<(Txid, Script)>,
    watched_outputs: Vec<WatchedOutput>,
}

impl TxFilter {
    fn new() -> Self {
        Self {
            watched_transactions: vec![],
            watched_outputs: vec![],
        }
    }

    fn register_tx(&mut self, txid: Txid, script: Script) {
        println!("Registering tx: {}", txid);
        self.watched_transactions.push((txid, script));
    }

    fn register_output(&mut self, output: WatchedOutput) {
        println!("watching output: {}", output.outpoint.txid);
        self.watched_outputs.push(output);
    }
}

impl Default for TxFilter {
    fn default() -> Self {
        Self::new()
    }
}

impl LipaLdk {
    /// Creates a new LDK instance
    ///
    /// Several tasks are launched into the tokio runtime handle that is provided.
    fn new(
        config: LipaLightningConfig,
        tokio_handle: &Handle,
        persist_callback: Arc<Box<dyn PersistCallback>>,
    ) -> Result<Self, ()> {
        let client = Arc::new(EsploraClient::new(&config.get_esplora_url()));

        // ## Setup

        info!("Setting up the node");

        // Step 1: Initialize the FeeEstimator
        let fee_estimator = Arc::new(EsploraClient::new(&config.get_esplora_url()));

        // Step 2: Initialize the Logger
        let logger = Arc::new(LightningLogger {});

        // Step 3: Initialize the BroadcasterInterface
        let broadcaster = client.clone();

        // Step 4: Initialize Persist
        let persister = Arc::new(LipaPersister::new(persist_callback.clone()));

        // Step 5: (Optional) Initialize the Transaction Filter
        // We won't use electrum yet so this isn't needed for now

        // Step 6: Initialize the ChainMonitor
        let filter = Arc::new(ChainFilter::new());
        let chain_monitor: Arc<ChainMonitor> = Arc::new(ChainMonitor::new(
            Some(filter.clone()),
            broadcaster.clone(),
            logger.clone(),
            fee_estimator.clone(),
            persister.clone(),
        ));

        // Step 7: Initialize the KeysManager using our seed
        if config.seed.len() != 32 {
            error!("Seed should have 32 bytes but has {}", config.seed.len());
        }
        let seed = <&[u8; 32]>::try_from(&*config.seed).unwrap();
        let cur = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap();
        let keys_manager = Arc::new(KeysManager::new(seed, cur.as_secs(), cur.subsec_nanos()));

        // Step 8: Read ChannelMonitor state from disk
        let mut channelmonitors = persister
            .read_channelmonitors(keys_manager.clone())
            .unwrap();

        // Step 9: Initialize the ChannelManager
        let mut user_config = UserConfig::default();

        // Do not announce outbound channels
        user_config.channel_handshake_config.announced_channel = false;
        // Enforce inbound channels to be unannounced channels
        user_config
            .channel_handshake_limits
            .force_announced_channel_preference = true;

        // todo implement accepting 0-conf inbound channels
        // required for 0-conf channels
        // user_config.manually_accept_inbound_channels = true;

        let channel_manager = {
            if persist_callback.exists("manager".to_string()) {
                let mut channel_monitor_mut_references = Vec::new();
                for (_, channel_monitor) in channelmonitors.iter_mut() {
                    channel_monitor_mut_references.push(channel_monitor);
                }
                let read_args = ChannelManagerReadArgs::new(
                    keys_manager.clone(),
                    fee_estimator.clone(),
                    chain_monitor.clone(),
                    broadcaster.clone(),
                    logger.clone(),
                    user_config,
                    channel_monitor_mut_references,
                );
                let mut ser_channelmanager = &*persist_callback.read("manager".to_string());
                let cm_persisted =
                    <(BlockHash, ChannelManager)>::read(&mut ser_channelmanager, read_args)
                        .unwrap();
                cm_persisted.1
            } else {
                // We're starting a fresh node.

                let tip_block_height = tokio_handle.block_on(client.get_height()).unwrap();
                let tip_block_hash = tokio_handle
                    .block_on(client.get_block_hash(tip_block_height))
                    .unwrap();

                let chain_params = ChainParameters {
                    network: config.network,
                    best_block: BestBlock::new(tip_block_hash, tip_block_height),
                };

                ChannelManager::new(
                    fee_estimator.clone(),
                    chain_monitor.clone(),
                    broadcaster.clone(),
                    logger.clone(),
                    keys_manager.clone(),
                    user_config,
                    chain_params,
                )
            }
        };

        // Step 12: (Optional) Initialize the NetGraphMsgHandler
        let genesis = genesis_block(config.network).header.block_hash();
        let network_graph_path = "network_graph".to_string();
        let network_graph = if !persist_callback.exists(network_graph_path.clone()) {
            Arc::new(NetworkGraph::new(genesis, logger.clone()))
        } else {
            let mut ser_network_graph = &*persist_callback.read(network_graph_path);
            Arc::new(NetworkGraph::read(&mut ser_network_graph, logger.clone()).unwrap())
        };
        let gossip_sync = Arc::new(P2PGossipSync::new(
            Arc::clone(&network_graph),
            None::<Arc<dyn chain::Access + Send + Sync>>,
            logger.clone(),
        ));

        // Step 13: Initialize the PeerManager
        let channel_manager: Arc<ChannelManager> = Arc::new(channel_manager);
        let mut ephemeral_bytes = [0; 32];
        rand::thread_rng().fill_bytes(&mut ephemeral_bytes);
        let lightning_msg_handler = MessageHandler {
            chan_handler: channel_manager.clone(),
            route_handler: gossip_sync.clone(),
        };
        let peer_manager: Arc<PeerManager> = Arc::new(PeerManager::new(
            lightning_msg_handler,
            keys_manager.get_node_secret(Recipient::Node).unwrap(),
            &ephemeral_bytes,
            logger.clone(),
            Arc::new(IgnoringMessageHandler {}),
        ));

        // ## Running LDK

        // Step 14: Initialize Networking
        let peer_manager_connection_handler = peer_manager.clone();
        let listening_port = config.ldk_peer_listening_port;
        let stop_listen_connect = Arc::new(AtomicBool::new(false));
        let stop_listen = Arc::clone(&stop_listen_connect);
        tokio_handle.spawn(async move {
            let listener = tokio::net::TcpListener::bind(format!("0.0.0.0:{}", listening_port))
                .await
                .expect(
                    "Failed to bind to listen port - is something else already listening on it?",
                );
            loop {
                let peer_mgr = peer_manager_connection_handler.clone();
                let tcp_stream = listener.accept().await.unwrap().0;
                if stop_listen.load(Ordering::Acquire) {
                    return;
                }
                tokio::spawn(async move {
                    lightning_net_tokio::setup_inbound(
                        peer_mgr.clone(),
                        tcp_stream.into_std().unwrap(),
                    )
                    .await;
                });
            }
        });

        // Step 16: Initialize an EventHandler
        // TODO: persist payment info to disk
        let inbound_payments: PaymentInfoStorage = Arc::new(Mutex::new(HashMap::new()));
        let outbound_payments: PaymentInfoStorage = Arc::new(Mutex::new(HashMap::new()));
        let network = config.network;
        let event_handler = Arc::new(LipaEventHandler {
            channel_manager: channel_manager.clone(),
            network_graph: network_graph.clone(),
            inbound_payments: inbound_payments.clone(),
            outbound_payments: outbound_payments.clone(),
            network,
            tokio_handle: tokio_handle.clone(),
        });

        // Step 17: Initialize the ProbabilisticScorer
        let scorer_path = "scorer".to_string();
        let scorer = if !persist_callback.exists(scorer_path.clone()) {
            Arc::new(Mutex::new(ProbabilisticScorer::new(
                ProbabilisticScoringParameters::default(),
                network_graph.clone(),
                logger.clone(),
            )))
        } else {
            let mut ser_scorer = &*persist_callback.read(scorer_path);
            let args = (
                ProbabilisticScoringParameters::default(),
                network_graph.clone(),
                logger.clone(),
            );
            Arc::new(Mutex::new(
                ProbabilisticScorer::read(&mut ser_scorer, args).unwrap(),
            ))
        };

        // Step 18: Initialize the InvoicePayer
        let router = DefaultRouter::new(
            network_graph.clone(),
            logger.clone(),
            keys_manager.get_secure_random_bytes(),
        );
        let invoice_payer = Arc::new(InvoicePayer::new(
            channel_manager.clone(),
            router,
            scorer.clone(),
            logger.clone(),
            event_handler,
            payment::Retry::Timeout(Duration::from_secs(10)),
        ));

        // Step 19: Initialize the Persister
        let persister = Arc::new(LipaPersister::new(persist_callback.clone()));

        // Step 20: Start background processor
        let background_processor = BackgroundProcessor::start(
            persister,
            invoice_payer.clone(),
            chain_monitor.clone(),
            channel_manager.clone(),
            GossipSync::p2p(gossip_sync.clone()),
            peer_manager.clone(),
            logger.clone(),
            Some(scorer.clone()),
        );

        // Regularly reconnect to channel peers.
        let connect_cm = Arc::clone(&channel_manager);
        let connect_pm = Arc::clone(&peer_manager);
        let peer_data_path = "channel_peer_data".to_string();
        let stop_connect = Arc::clone(&stop_listen_connect);
        let connect_pc = Arc::clone(&persist_callback);
        tokio_handle.spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(1));
            loop {
                interval.tick().await;
                // read channel peer data from disk
                if !connect_pc.exists(peer_data_path.clone()) {
                    continue;
                }
                let peer_pubkeys_str = connect_pc.read_dir(peer_data_path.clone());
                let peer_pubkeys: Vec<(PublicKey, String)> = peer_pubkeys_str
                    .iter()
                    .map(|pubkey_str| {
                        (
                            to_compressed_pubkey(pubkey_str).unwrap(),
                            pubkey_str.clone(),
                        )
                    })
                    .collect();

                let peers = connect_pm.get_peer_node_ids();
                for node_id in connect_cm
                    .list_channels()
                    .iter()
                    .map(|chan| chan.counterparty.node_id)
                    .filter(|id| !peers.contains(id))
                {
                    if stop_connect.load(Ordering::Acquire) {
                        return;
                    }
                    for (pubkey, pubkey_str) in peer_pubkeys.iter() {
                        if *pubkey == node_id {
                            let peer_addr_vec =
                                connect_pc.read(format!("{}/{}", peer_data_path, pubkey_str));
                            let peer_addr_str = &*String::from_utf8(peer_addr_vec)
                                .expect("ERROR: Failed to parse peer_addr into String");
                            let peer_addr = SocketAddr::from_str(peer_addr_str).expect(
                                "ERROR: Failed to parse peer_addr from String to SocketAddr",
                            );
                            let _ = do_connect_peer(*pubkey, peer_addr, connect_pm.clone()).await;
                        }
                    }
                }
            }
        });

        // We won't have public channels so we don't need to broadcast our node_announcement

        Ok(LipaLdk {
            client,
            stop_listen_connect,
            peer_manager,
            channel_manager,
            chain_monitor,
            background_processor,
            invoice_payer,
            inbound_payments,
            outbound_payments,
            network_graph,
            keys_manager,
            logger,
            scorer,
            network,
            filter,
        })
    }

    // pub(crate) fn sync_wallet(&self) -> Result<(), Error> {
    //     let sync_options = SyncOptions { progress: None };
    //
    //     self.wallet
    //         .lock()
    //         .unwrap()
    //         .sync(&self.blockchain, sync_options)
    //         .map_err(|e| Error::Bdk(e))?;
    //
    //     Ok(())
    // }

    pub(crate) async fn sync(&self) -> Result<(), Error> {
        let confirmables: Vec<Arc<dyn Confirm + Sync>> =
            vec![self.channel_manager.clone(), self.chain_monitor.clone()];

        let client = &*self.client;

        let cur_height = client.get_height().await?;

        // todo last_sync_height needs to be persisted
        // let mut locked_last_sync_height = self.last_sync_height.lock().unwrap();
        // Todo: mocking some value for now ...
        let current_sync_height_mock = Mutex::new(Some(10));

        let mut locked_last_sync_height = current_sync_height_mock.lock().unwrap();
        if cur_height >= locked_last_sync_height.unwrap_or(0) {
            {
                // First, inform the interface of the new block.
                let cur_block_header = client.get_header(cur_height).await?;

                for c in &confirmables {
                    c.best_block_updated(&cur_block_header, cur_height);
                }

                *locked_last_sync_height = Some(cur_height);
            }

            {
                // First, check the confirmation status of registered transactions as well as the
                // status of dependent transactions of registered outputs.
                // let mut locked_queued_transactions = self.queued_transactions.lock().unwrap();
                // let mut locked_queued_outputs = self.queued_outputs.lock().unwrap();
                // let mut locked_watched_transactions = self.watched_transactions.lock().unwrap();
                // let mut locked_watched_outputs = self.watched_outputs.lock().unwrap();

                let mut confirmed_txs = Vec::new();

                // Check in the current queue, as well as in registered transactions leftover from
                // previous iterations.
                // let mut registered_txs: Vec<Txid> = locked_watched_transactions
                //     .iter()
                //     .chain(locked_queued_transactions.iter())
                //     .cloned()
                //     .collect();

                let mut tx_filter = self.filter.filter.lock().unwrap();

                tx_filter
                    .watched_transactions
                    .sort_unstable_by(|txid1, txid2| txid1.cmp(&txid2));
                tx_filter
                    .watched_transactions
                    .dedup_by(|txid1, txid2| txid1.eq(&txid2));

                // Remember all registered but unconfirmed transactions for future processing.
                let mut unconfirmed_registered_txs = Vec::new();

                for txid in tx_filter.watched_transactions.iter() {
                    let txid = txid.0;
                    if let Some(tx_status) = client.get_tx_status(&txid).await? {
                        if tx_status.confirmed {
                            if let Some(tx) = client.get_tx(&txid).await? {
                                if let Some(block_height) = tx_status.block_height {
                                    let block_header = client.get_header(block_height).await?;
                                    if let Some(merkle_proof) =
                                        client.get_merkle_proof(&txid).await?
                                    {
                                        confirmed_txs.push((
                                            tx,
                                            block_height,
                                            block_header,
                                            merkle_proof.pos,
                                        ));
                                        continue;
                                    }
                                }
                            }
                        }
                    }
                    unconfirmed_registered_txs.push(txid);
                }

                // Check all registered outputs for dependent spending transactions.
                // let registered_outputs: Vec<WatchedOutput> = locked_watched_outputs
                //     .iter()
                //     .chain(locked_queued_outputs.iter())
                //     .cloned()
                //     .collect();

                // Remember all registered outputs that haven't been spent for future processing.
                let mut unspent_registered_outputs = Vec::new();

                for output in tx_filter.watched_outputs.iter() {
                    if let Some(output_status) = client
                        .get_output_status(&output.outpoint.txid, output.outpoint.index as u64)
                        .await?
                    {
                        if output_status.spent {
                            if let Some(spending_tx_status) = output_status.status {
                                if spending_tx_status.confirmed {
                                    let spending_txid = output_status.txid.unwrap();
                                    if let Some(spending_tx) = client.get_tx(&spending_txid).await?
                                    {
                                        let block_height = spending_tx_status.block_height.unwrap();
                                        let block_header = client.get_header(block_height).await?;
                                        if let Some(merkle_proof) =
                                            client.get_merkle_proof(&spending_txid).await?
                                        {
                                            confirmed_txs.push((
                                                spending_tx,
                                                block_height,
                                                block_header,
                                                merkle_proof.pos,
                                            ));
                                            continue;
                                        }
                                    }
                                }
                            }
                        }
                    }
                    unspent_registered_outputs.push(output);
                }

                // Sort all confirmed transactions by block height and feed them to the interface
                // in order.
                confirmed_txs.sort_unstable_by(
                    |(_, block_height1, _, _), (_, block_height2, _, _)| {
                        block_height1.cmp(&block_height2)
                    },
                );
                for (tx, block_height, block_header, pos) in confirmed_txs {
                    for c in &confirmables {
                        c.transactions_confirmed(&block_header, &[(pos, &tx)], block_height);
                    }
                }

                // *locked_watched_transactions = unconfirmed_registered_txs;
                // *locked_queued_transactions = Vec::new();
                // *locked_watched_outputs = unspent_registered_outputs;
                // *locked_queued_outputs = Vec::new();
            }

            {
                /* todo check for reorgs

                // Query the interface for relevant txids and check whether they have been
                // reorged-out of the chain.
                let unconfirmed_txids_unfiltered = confirmables
                    .iter()
                    .flat_map(|c| c.get_relevant_txids())
                    .collect::<Vec<Txid>>();

                let mut unconfirmed_txids = Vec::new();

                for unconfirmed_txid in unconfirmed_txids_unfiltered.iter() {
                    if client
                        .get_tx_status(unconfirmed_txid).await
                        .ok()
                        .unwrap_or(None)
                        .map_or(true, |status| !status.confirmed) {
                        unconfirmed_txids.push(*unconfirmed_txid);
                    }
                }

                // Mark all relevant unconfirmed transactions as unconfirmed.
                for txid in &unconfirmed_txids {
                    for c in &confirmables {
                        c.transaction_unconfirmed(txid);
                    }
                }

                 */
            }
        }

        // TODO: check whether new outputs have been registered by now and process them
        Ok(())
    }

    // pub(crate) fn create_funding_transaction(
    //     &self, output_script: &Script, value_sats: u64, confirmation_target: ConfirmationTarget,
    // ) -> Result<Transaction, Error> {
    //     let num_blocks = num_blocks_from_conf_target(confirmation_target);
    //     let fee_rate = self.blockchain.estimate_fee(num_blocks)?;
    //
    //     let locked_wallet = self.wallet.lock().unwrap();
    //     let mut tx_builder = locked_wallet.build_tx();
    //
    //     tx_builder.add_recipient(output_script.clone(), value_sats).fee_rate(fee_rate).enable_rbf();
    //
    //     let (mut psbt, _) = tx_builder.finish()?;
    //     log_trace!(self.logger, "Created funding PSBT: {:?}", psbt);
    //
    //     // We double-check that no inputs try to spend non-witness outputs. As we use a SegWit
    //     // wallet descriptor this technically shouldn't ever happen, but better safe than sorry.
    //     for input in &psbt.inputs {
    //         if input.witness_utxo.is_none() {
    //             return Err(Error::FundingTxNonWitnessOuputSpend);
    //         }
    //     }
    //
    //     let finalized = locked_wallet.sign(&mut psbt, SignOptions::default())?;
    //     if !finalized {
    //         return Err(Error::FundingTxNotFinalized);
    //     }
    //
    //     Ok(psbt.extract_tx())
    // }
    //
    // pub(crate) fn get_new_address(&self) -> Result<bitcoin::Address, Error> {
    //     let address_info = self.wallet.lock().unwrap().get_address(AddressIndex::New)?;
    //     Ok(address_info.address)
    // }
}

struct ChainFilter {
    filter: Mutex<TxFilter>,
}

impl ChainFilter {
    fn new() -> Self {
        Self {
            filter: Mutex::new(TxFilter::new()),
        }
    }
}

impl Filter for ChainFilter {
    fn register_tx(&self, txid: &Txid, script_pubkey: &Script) {
        let mut filter = self.filter.lock().unwrap();
        filter.register_tx(*txid, script_pubkey.clone());
    }

    fn register_output(&self, output: WatchedOutput) -> Option<TransactionWithPosition> {
        let mut filter = self.filter.lock().unwrap();
        filter.register_output(output);
        // TODO: do we need to check for tx here or wait for next sync?
        None
    }
}

pub fn init_native_logger_once(min_level: LogLevel) {
    native_logger::init_native_logger_once(min_level);
}

include!(concat!(env!("OUT_DIR"), "/lipalightninglib.uniffi.rs"));
